{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "## Project: Write an Algorithm for Landmark Classification\n",
    "\n",
    "\n",
    "---\n",
    "### Why We're Here\n",
    "\n",
    "Photo sharing and photo storage services like to have location data for each photo that is uploaded. With the location data, these services can build advanced features, such as automatic suggestion of relevant tags or automatic photo organization, which help provide a compelling user experience. Although a photo's location can often be obtained by looking at the photo's metadata, many photos uploaded to these services will not have location metadata available. This can happen when, for example, the camera capturing the picture does not have GPS or if a photo's metadata is scrubbed due to privacy concerns.\n",
    "\n",
    "If no location metadata for an image is available, one way to infer the location is to detect and classify a discernable landmark in the image. Given the large number of landmarks across the world and the immense volume of images that are uploaded to photo sharing services, using human judgement to classify these landmarks would not be feasible.\n",
    "\n",
    "In this notebook, you will take the first steps towards addressing this problem by building models to automatically predict the location of the image based on any landmarks depicted in the image. At the end of this project, your code will accept any user-supplied image as input and suggest the top k most relevant landmarks from 50 possible landmarks from across the world. The image below displays a potential sample output of your finished project.\n",
    "\n",
    "![Sample landmark classification output](images/sample_landmark_output.png)\n",
    "\n",
    "\n",
    "### The Road Ahead\n",
    "\n",
    "We break the notebook into separate steps.  Feel free to use the links below to navigate the notebook.\n",
    "\n",
    "* [Step 0](#step0): Download Datasets and Install Python Modules\n",
    "* [Step 1](#step1): Create a CNN to Classify Landmarks (from Scratch)\n",
    "* [Step 2](#step2): Create a CNN to Classify Landmarks (using Transfer Learning)\n",
    "* [Step 3](#step3): Write Your Landmark Prediction Algorithm\n",
    "\n",
    "---\n",
    "<a id='step0'></a>\n",
    "## Step 0: Download Datasets and Install Python Modules\n",
    "\n",
    "**Note: if you are using the Udacity workspace, *YOU CAN SKIP THIS STEP*. The dataset can be found in the `/data` folder and all required Python modules have been installed in the workspace.**\n",
    "\n",
    "Download the [landmark dataset](https://udacity-dlnfd.s3-us-west-1.amazonaws.com/datasets/landmark_images.zip).\n",
    "Unzip the folder and place it in this project's home directory, at the location `/landmark_images`.\n",
    "\n",
    "Install the following Python modules:\n",
    "* cv2\n",
    "* matplotlib\n",
    "* numpy\n",
    "* PIL\n",
    "* torch\n",
    "* torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='step1'></a>\n",
    "## Step 1: Create a CNN to Classify Landmarks (from Scratch)\n",
    "\n",
    "In this step, you will create a CNN that classifies landmarks.  You must create your CNN _from scratch_ (so, you can't use transfer learning _yet_!), and you must attain a test accuracy of at least 20%.\n",
    "\n",
    "Although 20% may seem low at first glance, it seems more reasonable after realizing how difficult of a problem this is. Many times, an image that is taken at a landmark captures a fairly mundane image of an animal or plant, like in the following picture.\n",
    "\n",
    "<img src=\"images/train/00.Haleakala_National_Park/084c2aa50d0a9249.jpg\" alt=\"Bird in Haleakalā National Park\" style=\"width: 400px;\"/>\n",
    "\n",
    "Just by looking at that image alone, would you have been able to guess that it was taken at the Haleakalā National Park in Hawaii?\n",
    "\n",
    "An accuracy of 20% is significantly better than random guessing, which would provide an accuracy of just 2%. In Step 2 of this notebook, you will have the opportunity to greatly improve accuracy by using transfer learning to create a CNN.\n",
    "\n",
    "Remember that practice is far ahead of theory in deep learning.  Experiment with many different architectures, and trust your intuition.  And, of course, have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Specify Data Loaders for the Landmark Dataset\n",
    "\n",
    "Use the code cell below to create three separate [data loaders](http://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader): one for training data, one for validation data, and one for test data. Randomly split the images located at `landmark_images/train` to create the train and validation data loaders, and use the images located at `landmark_images/test` to create the test data loader.\n",
    "\n",
    "**Note**: Remember that the dataset can be found at `/data/landmark_images/` in the workspace.\n",
    "\n",
    "All three of your data loaders should be accessible via a dictionary named `loaders_scratch`. Your train data loader should be at `loaders_scratch['train']`, your validation data loader should be at `loaders_scratch['valid']`, and your test data loader should be at `loaders_scratch['test']`.\n",
    "\n",
    "You may find [this documentation on custom datasets](https://pytorch.org/docs/stable/torchvision/datasets.html#datasetfolder) to be a useful resource.  If you are interested in augmenting your training and/or validation data, check out the wide variety of [transforms](http://pytorch.org/docs/stable/torchvision/transforms.html?highlight=transform)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next three cells were used to separate the training data from the validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt install unzip\n",
    "# !pip install split-folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip landmark_images.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import splitfolders\n",
    "# splitfolders.ratio('landmark_images/train', output=\"landmark_images/dataset\", seed=20, ratio=(.8, 0.2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "\n",
    "# convert data to a normalized torch.FloatTensor\n",
    "train_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(size=224),  # Image net standards\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "        transforms.Resize(size=224),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "train_data = ImageFolder('C:\\\\Users\\\\PC-1\\\\Documents\\\\GitHub\\\\nd101-c2-landmarks-starter\\\\landmark_project\\\\landmark_images\\\\train', transform=train_transform)\n",
    "val_data = ImageFolder('C:\\\\Users\\\\PC-1\\\\Documents\\\\GitHub\\\\nd101-c2-landmarks-starter\\\\landmark_project\\\\landmark_images\\\\test', transform=val_transform)\n",
    "#test_data = ImageFolder('landmark_images/test', transform=val_transform)\n",
    "\n",
    "loaders_scratch = {\n",
    "    'train': torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True),\n",
    "    'val': torch.utils.data.DataLoader(val_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "#    'test': torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "}\n",
    "\n",
    "classes = train_data.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** Describe your chosen procedure for preprocessing the data. \n",
    "- How does your code resize the images (by cropping, stretching, etc)?  What size did you pick for the input tensor, and why?\n",
    "- Did you decide to augment the dataset?  If so, how (through translations, flips, rotations, etc)?  If not, why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "Because the data for training and the valuation set was not separate, I separated it by separating the folders so that I could apply the data augmentation to the training data. I chose the image size to be 224 because the VGG16 accepts images of this size, so I want all models to be the same size to see the difference, as for the rest of the augmentation added to add more complexity to the trained data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Visualize a Batch of Training Data\n",
    "\n",
    "Use the code cell below to retrieve a batch of images from your train data loader, display at least 5 images simultaneously, and label each displayed image with its class name (e.g., \"Golden Gate Bridge\").\n",
    "\n",
    "Visualizing the output of your data loader is a great way to ensure that your data loading and preprocessing are working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im_convert(tensor):\n",
    "    \"\"\" Display a tensor as an image. \"\"\"\n",
    "    \n",
    "    image = tensor.to(\"cpu\").clone().detach()\n",
    "    image = image.numpy().squeeze()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    image = image.clip(0, 1)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(loaders_scratch['train']))\n",
    "images, labels = sample\n",
    "# display the images\n",
    "fig = plt.figure(figsize=(25, 10))\n",
    "# content and style ims side-by-side\n",
    "for i in range(6):\n",
    "    ax = fig.add_subplot(2, 6, i+1, xticks=[], yticks=[])\n",
    "    ax.imshow(im_convert(images[i]))\n",
    "    ax.set_title(classes[labels[i]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize use_cuda variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful variable that tells us whether we should use the GPU\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Specify Loss Function and Optimizer\n",
    "\n",
    "Use the next code cell to specify a [loss function](http://pytorch.org/docs/stable/nn.html#loss-functions) and [optimizer](http://pytorch.org/docs/stable/optim.html).  Save the chosen loss function as `criterion_scratch`, and fill in the function `get_optimizer_scratch` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_scratch = nn.CrossEntropyLoss()\n",
    "\n",
    "def get_optimizer_scratch(model):\n",
    "    optimizer_scratch = optim.SGD(model.parameters(), lr=0.005)\n",
    "    return optimizer_scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify images of landmarks.  Use the template in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        ## Define layers of a CNN\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 8, 3, padding=1)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(8, 64, 3, padding=1)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        \n",
    "        # max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        \n",
    "        self.fc1 = nn.Linear(512 * 7 * 7, 5000)\n",
    "        \n",
    "        self.fc2 = nn.Linear(5000, 2500)\n",
    "       \n",
    "        self.fc3 = nn.Linear(2500, 500)\n",
    "      \n",
    "        self.fc4 = nn.Linear(500, 133)\n",
    "        \n",
    "        self.fc5 = nn.Linear(133, 50)\n",
    "\n",
    "        # dropout layer (p=0.25)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## Define forward behavior\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = self.pool(F.relu(self.conv5(x)))\n",
    "        \n",
    "        \n",
    "        # flatten image input\n",
    "        x = x.view(-1, 512 * 7 * 7)\n",
    "        \n",
    "        # fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# instantiate the CNN\n",
    "model_scratch = Net()\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if use_cuda:\n",
    "    model_scratch.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        ## Define layers of a CNN\n",
    "        self.conv1 = nn.Conv2d(3, 64, 11, stride=4, padding=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 192, 5, padding=2)\n",
    "       \n",
    "        self.conv3 = nn.Conv2d(192, 256, 3, padding=2)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(256, 512, 3, padding=2)\n",
    "       \n",
    "        \n",
    "        \n",
    "        # max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(512 * 4 * 4, 5000)\n",
    "        \n",
    "        self.fc2 = nn.Linear(5000, 2500)\n",
    "       \n",
    "        self.fc3 = nn.Linear(2500, 500)\n",
    "      \n",
    "        self.fc4 = nn.Linear(500, 133)\n",
    "        # dropout layer (p=0.25)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        # batch normalization\n",
    "        self.batn = nn.BatchNorm2d(192)\n",
    "        self.batn2 = nn.BatchNorm2d(256)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## Define forward behavior\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.batn(x)\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.batn2(x)\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        \n",
    "        \n",
    "        # flatten image input\n",
    "        x = x.view(-1, 512 * 4 * 4)\n",
    "        \n",
    "        # fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# instantiate the CNN\n",
    "model_scratch = Net()\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if use_cuda:\n",
    "    model_scratch.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__  \n",
    "The model consists of four layers of CNN interspersed with BatchNorm2d to avoid explosion or decay, followed by four Fully Connected layers, between each layer and the other a drop by 25% to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Implement the Training Algorithm\n",
    "\n",
    "Implement your training algorithm in the code cell below.  [Save the final model parameters](http://pytorch.org/docs/master/notes/serialization.html) at the filepath stored in the variable `save_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        # set the module to training mode\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            ## TODO: find the loss and update the model parameters accordingly\n",
    "            ## record the average training loss, using something like\n",
    "            ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data.item() - train_loss))\n",
    "\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            ## record the average training loss, using something like\n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            \n",
    "        ######################\n",
    "        # validate the model #\n",
    "        ######################\n",
    "        # set the model to evaluation mode\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "\n",
    "\n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "\n",
    "\n",
    "        if valid_loss < valid_loss_min:\n",
    "                    torch.save(model.state_dict(), save_path)\n",
    "\n",
    "                    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                    valid_loss_min,\n",
    "                    valid_loss))\n",
    "\n",
    "                    valid_loss_min = valid_loss\n",
    "        \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Experiment with the Weight Initialization\n",
    "\n",
    "Use the code cell below to define a custom weight initialization, and then train with your weight initialization for a few epochs. Make sure that neither the training loss nor validation loss is `nan`.\n",
    "\n",
    "Later on, you will be able to see how this compares to training with PyTorch's default weight initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_weight_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        # get the number of the inputs\n",
    "        n = m.in_features\n",
    "        y = (1.0/np.sqrt(n))\n",
    "        m.weight.data.normal_(0, y)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "model_scratch.apply(custom_weight_init)\n",
    "model_scratch = train(20, loaders_scratch, model_scratch, get_optimizer_scratch(model_scratch),\n",
    "                      criterion_scratch, use_cuda, 'ignore.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model with the Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loaders, model, criterion, use_cuda):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    # set the module to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data.item() - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model that got the best validation accuracy\n",
    "model_scratch.load_state_dict(torch.load('ignore.pt'))\n",
    "test(loaders_scratch, model_scratch, criterion_scratch, use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train and Validate the Model\n",
    "\n",
    "Run the next code cell to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## but changing it is not required\n",
    "num_epochs = 25\n",
    "\n",
    "\n",
    "# function to re-initialize a model with pytorch's default weight initialization\n",
    "def default_weight_init(m):\n",
    "    reset_parameters = getattr(m, 'reset_parameters', None)\n",
    "    if callable(reset_parameters):\n",
    "        m.reset_parameters()\n",
    "\n",
    "# reset the model parameters\n",
    "model_scratch.apply(default_weight_init)\n",
    "\n",
    "# train the model\n",
    "model_scratch = train(num_epochs, loaders_scratch, model_scratch, get_optimizer_scratch(model_scratch), \n",
    "                      criterion_scratch, use_cuda, 'model_scratch.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Test the Model\n",
    "\n",
    "Run the code cell below to try out your model on the test dataset of landmark images. Run the code cell below to calculate and print the test loss and accuracy.  Ensure that your test accuracy is greater than 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model that got the best validation accuracy\n",
    "model_scratch.load_state_dict(torch.load('model_scratch.pt'))\n",
    "test(loaders_scratch, model_scratch, criterion_scratch, use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step2'></a>\n",
    "## Step 2: Create a CNN to Classify Landmarks (using Transfer Learning)\n",
    "\n",
    "You will now use transfer learning to create a CNN that can identify landmarks from images.  Your CNN must attain at least 60% accuracy on the test set.\n",
    "\n",
    "### (IMPLEMENTATION) Specify Data Loaders for the Landmark Dataset\n",
    "\n",
    "Use the code cell below to create three separate [data loaders](http://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader): one for training data, one for validation data, and one for test data. Randomly split the images located at `landmark_images/train` to create the train and validation data loaders, and use the images located at `landmark_images/test` to create the test data loader.\n",
    "\n",
    "All three of your data loaders should be accessible via a dictionary named `loaders_transfer`. Your train data loader should be at `loaders_transfer['train']`, your validation data loader should be at `loaders_transfer['valid']`, and your test data loader should be at `loaders_transfer['test']`.\n",
    "\n",
    "If you like, **you are welcome to use the same data loaders from the previous step**, when you created a CNN from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify appropriate transforms, and batch_sizes\n",
    "\n",
    "loaders_transfer = loaders_scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Specify Loss Function and Optimizer\n",
    "\n",
    "Use the next code cell to specify a [loss function](http://pytorch.org/docs/stable/nn.html#loss-functions) and [optimizer](http://pytorch.org/docs/stable/optim.html).  Save the chosen loss function as `criterion_transfer`, and fill in the function `get_optimizer_transfer` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_transfer = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def get_optimizer_transfer(model):\n",
    "    ptimizer_transfer =optim.SGD(model.classifier.parameters(), lr=0.005)\n",
    "    return ptimizer_transfer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Use transfer learning to create a CNN to classify images of landmarks.  Use the code cell below, and save your initialized model as the variable `model_transfer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "model_transfer = models.vgg16(pretrained=True)\n",
    "\n",
    "\n",
    "# Freeze training for all \"features\" layers\n",
    "for param in model_transfer.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "n_inputs = model_transfer.classifier[6].in_features\n",
    "\n",
    "# add last linear layer (n_inputs -> 5 flower classes)\n",
    "# new layers automatically have requires_grad = True\n",
    "last_layer = nn.Linear(n_inputs, len(classes))\n",
    "\n",
    "model_transfer.classifier[6] = last_layer\n",
    "\n",
    "\n",
    "\n",
    "if use_cuda:\n",
    "    model_transfer = model_transfer.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_transfer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 3:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  Describe why you think the architecture is suitable for the current problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__  \n",
    "As for the first model, I think it is suitable because it has several layers, which allows to extract more features, but it needs a lot of time to train to reach an acceptable score, but we can save time by useing VGG16 iit is a pre-trained before, so we see this big difference between the scores.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train and Validate the Model\n",
    "\n",
    "Train and validate your model in the code cell below.  [Save the final model parameters](http://pytorch.org/docs/master/notes/serialization.html) at filepath `'model_transfer.pt'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train the model\n",
    "n_epochs=30\n",
    "\n",
    "model_transfer = train(n_epochs, loaders_transfer, model_transfer, \n",
    "                       get_optimizer_transfer(model_transfer), criterion_transfer, use_cuda, 'model_transfer.pt')\n",
    "\n",
    "\n",
    "\n",
    "# load the model that got the best validation accuracy\n",
    "model_transfer.load_state_dict(torch.load('model_transfer.pt'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Test the Model\n",
    "\n",
    "Try out your model on the test dataset of landmark images. Use the code cell below to calculate and print the test loss and accuracy.  Ensure that your test accuracy is greater than 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(loaders_transfer, model_transfer, criterion_transfer, use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step3'></a>\n",
    "## Step 3: Write Your Landmark Prediction Algorithm\n",
    "\n",
    "Great job creating your CNN models! Now that you have put in all the hard work of creating accurate classifiers, let's define some functions to make it easy for others to use your classifiers.\n",
    "\n",
    "### (IMPLEMENTATION) Write Your Algorithm, Part 1\n",
    "\n",
    "Implement the function `predict_landmarks`, which accepts a file path to an image and an integer k, and then predicts the **top k most likely landmarks**. You are **required** to use your transfer learned CNN from Step 2 to predict the landmarks.\n",
    "\n",
    "An example of the expected behavior of `predict_landmarks`:\n",
    "```\n",
    ">>> predicted_landmarks = predict_landmarks('example_image.jpg', 3)\n",
    ">>> print(predicted_landmarks)\n",
    "['Golden Gate Bridge', 'Brooklyn Bridge', 'Sydney Harbour Bridge']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "## the class names can be accessed at the `classes` attribute\n",
    "## of your dataset object (e.g., `train_dataset.classes`)\n",
    "\n",
    "def predict_landmarks(img_path, k):\n",
    "        # load the image and return the predicted breed\n",
    "    img=Image.open(img_path)\n",
    "    \n",
    "    transform=transforms.Compose([ transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "    #from image to tensor\n",
    "    image_tensor = transform(img).float()\n",
    "    #adds a dimension with a length of one\n",
    "    image_ten = image_tensor.unsqueeze_(0)\n",
    "    \n",
    "    if use_cuda:\n",
    "        img=image_ten.cuda()\n",
    "        \n",
    "    model_transfer.eval()  #evaluation mode \n",
    "    output = model_transfer(img)\n",
    "    _, pred = torch.topk(output, k, dim=1, largest=True, sorted=True)\n",
    "\n",
    "    return pred\n",
    "    \n",
    "\n",
    "\n",
    "# test on a sample image\n",
    "predict_landmarks('images/test/09.Golden_Gate_Bridge/190f3bae17c32c37.jpg', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Write Your Algorithm, Part 2\n",
    "\n",
    "In the code cell below, implement the function `suggest_locations`, which accepts a file path to an image as input, and then displays the image and the **top 3 most likely landmarks** as predicted by `predict_landmarks`.\n",
    "\n",
    "Some sample output for `suggest_locations` is provided below, but feel free to design your own user experience!\n",
    "![](images/sample_landmark_output.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [item[3:].replace(\"_\", \" \") for item in classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(loaders_scratch['test'])\n",
    "images, labels = dataiter.next()\n",
    "images.numpy()\n",
    "def suggest_locations(img_path):\n",
    "    # get landmark predictions\n",
    "    predicted_landmarks = predict_landmarks(img_path, 3)\n",
    "    \n",
    "    if \"http\" in img_path:\n",
    "        response = requests.get(img_path)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "    in_transform = transforms.Compose([\n",
    "                        transforms.Resize(224),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                                             (0.229, 0.224, 0.225))])\n",
    "\n",
    "    # discard the transparent, alpha channel (that's the :3) and add the batch dimension\n",
    "    \n",
    "    image = in_transform(image)[:3,:,:].unsqueeze(0)\n",
    "    image = plt.imshow(im_convert(image))\n",
    "    location = print(f\"Is this picture of the \\n{class_names[predicted_landmarks[0][0]]}, {class_names[predicted_landmarks[0][1]]},or {class_names[predicted_landmarks[0][2]]} ?\" )\n",
    "    return location, image\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# test on a sample image\n",
    "suggest_locations('images/test/09.Golden_Gate_Bridge/190f3bae17c32c37.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Test Your Algorithm\n",
    "\n",
    "Test your algorithm by running the `suggest_locations` function on at least four images on your computer. Feel free to use any images you like.\n",
    "\n",
    "__Question 4:__ Is the output better than you expected :) ?  Or worse :( ?  Provide at least three possible points of improvement for your algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__ (Three possible points for improvement)\n",
    "The model recognized the image of the Eagle Tower, although it is not in an ideal situation, but he did not recognize the rest because it is outside the classes he trained in.\n",
    "Therefore, to develop it, I see that the number of classes should be increased, as well as training the model for a longer period and more data for each classes so that the model is trained in batches.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggest_locations('suggest_locations/ezgif.com-gif-maker.jpg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggest_locations('suggest_locations/pyramids-of-giza-egypt.jpg.jpg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggest_locations('suggest_locations/Temple-Expiatori-de-la-Sagrada-Familia-Barcelona-Spain.jpg.jpg');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-f294f163",
   "language": "python",
   "display_name": "PyCharm (deep-learning)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}